{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3918607f-da82-445d-8bd2-2f980d9b036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apriori algorithm\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(val):\n",
    "        if val == 't':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "class CSVLoader():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    # load csv\n",
    "\n",
    "    def load_csv(self, csv_name: str) -> DataFrame:\n",
    "        self.column_list = pd.read_csv(csv_name, index_col=0, nrows=0).columns.tolist()\n",
    "        csv_header_len = len(self.column_list) + 1\n",
    "        csv_file = pd.read_csv(csv_name, engine=\"python\", converters={k: conv for k in range(csv_header_len)})\n",
    "        return csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bcf02fe-eaf6-47ac-b5cb-a96abf281c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "605\n",
      "264\n",
      "12\n",
      "Number of rules:  22 \n",
      "\n",
      "{'bottled_water', 'butter'} -> {'whole_milk'} <confidence: 0.6022727272727273>\n",
      "{'butter', 'yogurt'} -> {'whole_milk'} <confidence: 0.6388888888888888>\n",
      "{'pip_fruit', 'domestic_eggs'} -> {'whole_milk'} <confidence: 0.6235294117647059>\n",
      "{'pip_fruit', 'whipped_sour_cream'} -> {'other_vegetables'} <confidence: 0.6043956043956045>\n",
      "{'curd', 'tropical_fruit'} -> {'whole_milk'} <confidence: 0.6336633663366337>\n",
      "{'butter', 'tropical_fruit'} -> {'whole_milk'} <confidence: 0.6224489795918368>\n",
      "{'margarine', 'domestic_eggs'} -> {'whole_milk'} <confidence: 0.6219512195121952>\n",
      "{'butter', 'whipped_sour_cream'} -> {'whole_milk'} <confidence: 0.66>\n",
      "{'domestic_eggs', 'tropical_fruit'} -> {'whole_milk'} <confidence: 0.6071428571428571>\n",
      "{'butter', 'domestic_eggs'} -> {'whole_milk'} <confidence: 0.6210526315789474>\n",
      "{'butter', 'root_vegetables'} -> {'whole_milk'} <confidence: 0.6377952755905512>\n",
      "{'root_vegetables', 'onions'} -> {'other_vegetables'} <confidence: 0.6021505376344086>\n",
      "{'pip_fruit', 'whipped_sour_cream'} -> {'whole_milk'} <confidence: 0.6483516483516485>\n",
      "{'pip_fruit', 'other_vegetables', 'root_vegetables'} -> {'whole_milk'} <confidence: 0.675>\n",
      "{'pip_fruit', 'whole_milk', 'root_vegetables'} -> {'other_vegetables'} <confidence: 0.6136363636363636>\n",
      "{'other_vegetables', 'tropical_fruit', 'yogurt'} -> {'whole_milk'} <confidence: 0.6198347107438016>\n",
      "{'whipped_sour_cream', 'other_vegetables', 'root_vegetables'} -> {'whole_milk'} <confidence: 0.6071428571428571>\n",
      "{'pip_fruit', 'other_vegetables', 'yogurt'} -> {'whole_milk'} <confidence: 0.625>\n",
      "{'root_vegetables', 'tropical_fruit', 'yogurt'} -> {'whole_milk'} <confidence: 0.7000000000000001>\n",
      "{'root_vegetables', 'citrus_fruit', 'whole_milk'} -> {'other_vegetables'} <confidence: 0.6333333333333333>\n",
      "{'other_vegetables', 'fruit_vegetable_juice', 'yogurt'} -> {'whole_milk'} <confidence: 0.6172839506172838>\n",
      "{'other_vegetables', 'root_vegetables', 'yogurt'} -> {'whole_milk'} <confidence: 0.6062992125984252>\n"
     ]
    }
   ],
   "source": [
    "class Apriori():\n",
    "\n",
    "    def __init__(self, support: float, confidence: float, csv_name: str) -> None:\n",
    "        csv_loader = CSVLoader()\n",
    "        self.data = csv_loader.load_csv(csv_name=csv_name)\n",
    "        self.min_support = support\n",
    "        self.confidence = confidence\n",
    "        self.row_count = len(self.data.index)\n",
    "\n",
    "    def calculate_support(self, data: DataFrame) -> dict:\n",
    "        support_count = {item:0 for item in data.columns}\n",
    "        for column in data.columns:\n",
    "            support_count[column] = data[column].value_counts()[1]/self.row_count\n",
    "        return support_count\n",
    "\n",
    "    def prune(self, current_combinations: Iterable, previous_frequent_combinations: list, previous_length: int) -> list:\n",
    "        # remove all items from set that are not part of previous frequent list\n",
    "        # combinations = [(1,2,3), ... , (1,2,3)]\n",
    "        new_combinations_list = []\n",
    "        if previous_length == 1:\n",
    "            previous_frequent_combinations = [{item} for item in previous_frequent_combinations]\n",
    "        for index, item in enumerate(current_combinations):\n",
    "            temp_combinations = list(itertools.combinations(item, previous_length))\n",
    "            for prev_item in temp_combinations:\n",
    "                temp_set = set(prev_item)\n",
    "                if temp_set in previous_frequent_combinations:\n",
    "                    new_combinations_list.append(item)   \n",
    "        return new_combinations_list\n",
    "\n",
    "    def above_min_support(self, frequency_dict: dict) -> list:\n",
    "        above_min_support = [{key: support} for key, support in frequency_dict.items() if support > self.min_support]\n",
    "        return above_min_support\n",
    "\n",
    "    def apriori(self) -> list:\n",
    "        c1 = self.calculate_support(self.data)\n",
    "        l1 = self.above_min_support(c1)\n",
    "\n",
    "        accumulated_frequent_dataset = {}\n",
    "\n",
    "        current_lset = l1\n",
    "        current_iteration = 2\n",
    "        while current_lset:\n",
    "            print(len(current_lset))\n",
    "\n",
    "            accumulated_frequent_dataset[current_iteration - 1] = current_lset\n",
    "            # create combinations\n",
    "            if current_iteration - 1 == 1:\n",
    "                current_lset = list(set(list(np.array([list(item.keys()) for item in current_lset]).ravel())))\n",
    "            else:\n",
    "                new_lset = []\n",
    "                for item in current_lset:\n",
    "                    for key in item.keys():\n",
    "                        temp_keys = {new_item for new_item in key}\n",
    "                        new_lset.append(temp_keys)\n",
    "                current_lset = new_lset\n",
    "            # print(current_lset)\n",
    "            if current_iteration == 2:\n",
    "                combinations = itertools.combinations(current_lset, current_iteration)\n",
    "            else:\n",
    "                new_combinations = []\n",
    "                for item_test in current_lset:\n",
    "                    for sub_item_test in current_lset:\n",
    "                        union_set = item_test.union(sub_item_test)\n",
    "                        # print(union_set)\n",
    "                        if len(union_set) == current_iteration:\n",
    "                            new_combinations.append(tuple(union_set))\n",
    "                \n",
    "                combinations =  [frozenset(s) for s in new_combinations]\n",
    "                combinations = set(combinations)\n",
    "                combinations = [set(item) for item in combinations]\n",
    "                combinations = [tuple(item) for item in combinations]\n",
    "            combinations = self.prune(current_combinations=combinations, previous_frequent_combinations=current_lset, previous_length=current_iteration-1)\n",
    "            # calculate above min support\n",
    "            current_c = {}\n",
    "            count = 0\n",
    "            for item in combinations:\n",
    "                condition = np.all(self.data[list(item)], axis=1)\n",
    "                count = np.sum(condition)\n",
    "                current_c[item] = count/self.row_count\n",
    "                count = 0\n",
    "            current_lset = self.above_min_support(current_c)\n",
    "            current_iteration += 1\n",
    "        return accumulated_frequent_dataset\n",
    "\n",
    "    def find_subset(item, item_length):\n",
    "        combs = []\n",
    "        for i in range(1, item_length + 1):\n",
    "            combs.append(list(itertools.combinations(item, i)))\n",
    "            \n",
    "        subsets = []\n",
    "        for comb in combs:\n",
    "            for elt in comb:\n",
    "                subsets.append(elt)\n",
    "                \n",
    "        return subsets\n",
    "\n",
    "    def association_rules(self, acc_dataset: dict) -> None:\n",
    "        rules = list()\n",
    "        for item, support in acc_dataset.items():\n",
    "            item_length = len(item)\n",
    "            \n",
    "            if item_length > 1:\n",
    "                subsets = find_subset(item, item_length)\n",
    "            \n",
    "                for A in subsets:\n",
    "                    B = item.difference(A)\n",
    "                \n",
    "                    if B:\n",
    "                        A = frozenset(A)\n",
    "                        \n",
    "                        AB = A | B\n",
    "                        \n",
    "                        confidence = acc_dataset[AB] / acc_dataset[A]\n",
    "                        if confidence >= self.confidence:\n",
    "                            rules.append((A, B, confidence))\n",
    "\n",
    "        # print(rules)\n",
    "        print(\"Number of rules: \", len(rules), \"\\n\")\n",
    "\n",
    "        for rule in rules:\n",
    "            print('{0} -> {1} <confidence: {2}>'.format(set(rule[0]), set(rule[1]), rule[2]))\n",
    "\n",
    "def main():\n",
    "    support = 0.005\n",
    "    confidence = 0.6\n",
    "    csv_name = \"./myDataFile.csv\"\n",
    "    apriori = Apriori(support=support, confidence=confidence, csv_name=csv_name)\n",
    "    acc_dataset = apriori.apriori()\n",
    "    result_dict = {}\n",
    "    for i in range(1,len(acc_dataset) + 1):\n",
    "        # print(i)\n",
    "        for sub_item in acc_dataset[i]:\n",
    "            # print(frozenset(sub_item.keys()))\n",
    "            result_dict.update(sub_item)\n",
    "    processed_result_dict = {}\n",
    "    for key in result_dict.keys():\n",
    "        new_tuple = key\n",
    "        if isinstance(key,str):\n",
    "            new_tuple = [key]\n",
    "        processed_result_dict[frozenset(new_tuple)] = result_dict[key]\n",
    "    # print(processed_result_dict)\n",
    "    apriori.association_rules(processed_result_dict)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
