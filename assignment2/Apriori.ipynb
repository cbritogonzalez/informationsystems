{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3918607f-da82-445d-8bd2-2f980d9b036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apriori algorithm\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(val):\n",
    "        if val == 't':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "class CSVLoader():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    # load csv\n",
    "\n",
    "    def load_csv(self, csv_name: str) -> DataFrame:\n",
    "        self.column_list = pd.read_csv(csv_name, index_col=0, nrows=0).columns.tolist()\n",
    "        csv_header_len = len(self.column_list) + 1\n",
    "        csv_file = pd.read_csv(csv_name, engine=\"python\", converters={k: conv for k in range(csv_header_len)})\n",
    "        return csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcf02fe-eaf6-47ac-b5cb-a96abf281c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "605\n",
      "264\n",
      "12\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'find_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 146\u001b[0m\n\u001b[1;32m    142\u001b[0m     apriori\u001b[38;5;241m.\u001b[39massociation_rules(processed_result_dict)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 142\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     processed_result_dict[\u001b[38;5;28mfrozenset\u001b[39m(new_tuple)] \u001b[38;5;241m=\u001b[39m result_dict[key]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# print(processed_result_dict)\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m \u001b[43mapriori\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massociation_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_result_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mApriori.association_rules\u001b[0;34m(self, acc_dataset)\u001b[0m\n\u001b[1;32m    100\u001b[0m item_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(item)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     subsets \u001b[38;5;241m=\u001b[39m \u001b[43mfind_subset\u001b[49m(item, item_length)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m A \u001b[38;5;129;01min\u001b[39;00m subsets:\n\u001b[1;32m    106\u001b[0m         B \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mdifference(A)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_subset' is not defined"
     ]
    }
   ],
   "source": [
    "class Apriori():\n",
    "\n",
    "    def __init__(self, support: float, confidence: float, csv_name: str) -> None:\n",
    "        csv_loader = CSVLoader()\n",
    "        self.data = csv_loader.load_csv(csv_name=csv_name)\n",
    "        self.min_support = support\n",
    "        self.confidence = confidence\n",
    "        self.row_count = len(self.data.index)\n",
    "\n",
    "    def calculate_support(self, data: DataFrame) -> dict:\n",
    "        support_count = {item:0 for item in data.columns}\n",
    "        for column in data.columns:\n",
    "            support_count[column] = data[column].value_counts()[1]/self.row_count\n",
    "        return support_count\n",
    "\n",
    "    def prune(self, current_combinations: Iterable, previous_frequent_combinations: list, previous_length: int) -> list:\n",
    "        # remove all items from set that are not part of previous frequent list\n",
    "        # combinations = [(1,2,3), ... , (1,2,3)]\n",
    "        new_combinations_list = []\n",
    "        if previous_length == 1:\n",
    "            previous_frequent_combinations = [{item} for item in previous_frequent_combinations]\n",
    "        for index, item in enumerate(current_combinations):\n",
    "            temp_combinations = list(itertools.combinations(item, previous_length))\n",
    "            for prev_item in temp_combinations:\n",
    "                temp_set = set(prev_item)\n",
    "                if temp_set in previous_frequent_combinations:\n",
    "                    new_combinations_list.append(item)   \n",
    "        return new_combinations_list\n",
    "\n",
    "    def above_min_support(self, frequency_dict: dict) -> list:\n",
    "        above_min_support = [{key: support} for key, support in frequency_dict.items() if support > self.min_support]\n",
    "        return above_min_support\n",
    "\n",
    "    def apriori(self) -> list:\n",
    "        c1 = self.calculate_support(self.data)\n",
    "        l1 = self.above_min_support(c1)\n",
    "\n",
    "        accumulated_frequent_dataset = {}\n",
    "\n",
    "        current_lset = l1\n",
    "        current_iteration = 2\n",
    "        while current_lset:\n",
    "            print(len(current_lset))\n",
    "\n",
    "            accumulated_frequent_dataset[current_iteration - 1] = current_lset\n",
    "            # create combinations\n",
    "            if current_iteration - 1 == 1:\n",
    "                current_lset = list(set(list(np.array([list(item.keys()) for item in current_lset]).ravel())))\n",
    "            else:\n",
    "                new_lset = []\n",
    "                for item in current_lset:\n",
    "                    for key in item.keys():\n",
    "                        temp_keys = {new_item for new_item in key}\n",
    "                        new_lset.append(temp_keys)\n",
    "                current_lset = new_lset\n",
    "            # print(current_lset)\n",
    "            if current_iteration == 2:\n",
    "                combinations = itertools.combinations(current_lset, current_iteration)\n",
    "            else:\n",
    "                new_combinations = []\n",
    "                for item_test in current_lset:\n",
    "                    for sub_item_test in current_lset:\n",
    "                        union_set = item_test.union(sub_item_test)\n",
    "                        # print(union_set)\n",
    "                        if len(union_set) == current_iteration:\n",
    "                            new_combinations.append(tuple(union_set))\n",
    "                \n",
    "                combinations =  [frozenset(s) for s in new_combinations]\n",
    "                combinations = set(combinations)\n",
    "                combinations = [set(item) for item in combinations]\n",
    "                combinations = [tuple(item) for item in combinations]\n",
    "            combinations = self.prune(current_combinations=combinations, previous_frequent_combinations=current_lset, previous_length=current_iteration-1)\n",
    "            # calculate above min support\n",
    "            current_c = {}\n",
    "            count = 0\n",
    "            for item in combinations:\n",
    "                condition = np.all(self.data[list(item)], axis=1)\n",
    "                count = np.sum(condition)\n",
    "                current_c[item] = count/self.row_count\n",
    "                count = 0\n",
    "            current_lset = self.above_min_support(current_c)\n",
    "            current_iteration += 1\n",
    "        return accumulated_frequent_dataset\n",
    "\n",
    "    def find_subset(self, item, item_length):\n",
    "        combs = []\n",
    "        for i in range(1, item_length + 1):\n",
    "            combs.append(list(itertools.combinations(item, i)))\n",
    "            \n",
    "        subsets = []\n",
    "        for comb in combs:\n",
    "            for elt in comb:\n",
    "                subsets.append(elt)\n",
    "                \n",
    "        return subsets\n",
    "\n",
    "    def association_rules(self, acc_dataset: dict) -> None:\n",
    "        rules = list()\n",
    "        for item, support in acc_dataset.items():\n",
    "            item_length = len(item)\n",
    "            \n",
    "            if item_length > 1:\n",
    "                subsets = self.find_subset(item, item_length)\n",
    "            \n",
    "                for A in subsets:\n",
    "                    B = item.difference(A)\n",
    "                \n",
    "                    if B:\n",
    "                        A = frozenset(A)\n",
    "                        \n",
    "                        AB = A | B\n",
    "                        \n",
    "                        confidence = acc_dataset[AB] / acc_dataset[A]\n",
    "                        if confidence >= self.confidence:\n",
    "                            rules.append((A, B, confidence))\n",
    "\n",
    "        # print(rules)\n",
    "        print(\"Number of rules: \", len(rules), \"\\n\")\n",
    "\n",
    "        for rule in rules:\n",
    "            print('{0} -> {1} <confidence: {2}>'.format(set(rule[0]), set(rule[1]), rule[2]))\n",
    "\n",
    "def main():\n",
    "    support = 0.005\n",
    "    confidence = 0.6\n",
    "    csv_name = \"./myDataFile.csv\"\n",
    "    apriori = Apriori(support=support, confidence=confidence, csv_name=csv_name)\n",
    "    acc_dataset = apriori.apriori()\n",
    "    result_dict = {}\n",
    "    for i in range(1,len(acc_dataset) + 1):\n",
    "        # print(i)\n",
    "        for sub_item in acc_dataset[i]:\n",
    "            # print(frozenset(sub_item.keys()))\n",
    "            result_dict.update(sub_item)\n",
    "    processed_result_dict = {}\n",
    "    for key in result_dict.keys():\n",
    "        new_tuple = key\n",
    "        if isinstance(key,str):\n",
    "            new_tuple = [key]\n",
    "        processed_result_dict[frozenset(new_tuple)] = result_dict[key]\n",
    "    # print(processed_result_dict)\n",
    "    apriori.association_rules(processed_result_dict)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
